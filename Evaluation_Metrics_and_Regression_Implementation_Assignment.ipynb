{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDTOb+MnFjYqlIwC6nUTJo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ambar209/Ambar209/blob/main/Evaluation_Metrics_and_Regression_Implementation_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utSDq2v8vef8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "                            **Theoretical**\n",
        "Q1: What does R-squared represent in a regression model\n",
        "\n",
        "Answer: R-squared represents how well the regression model explains the\n",
        "        variation in the dependent variable.\n",
        "\n",
        "Q2: What are the assumptions of linear regression\n",
        "\n",
        "Answer: Linearity: The relationship between the independent and dependent\n",
        "        variables is linear.\n",
        "\n",
        "        Independence: The residuals (errors) are independent of each other.\n",
        "\n",
        "        Homoscedasticity: The residuals have constant variance across all levels of the independent variable.\n",
        "\n",
        "Q3: What is the difference between R-squared and Adjusted R-squared\n",
        "\n",
        "Answer: R-squared shows how much of the variability in the dependent variable  \n",
        "        is explained by the model, but it can increase even with unnecessary variables. Adjusted R-squared adjusts for the number of variables, so it gives a more accurate measure of model fit, especially when adding more predictors. It can decrease if extra variables dont improve the model.\n",
        "\n",
        "Q4:  Why do we use Mean Squared Error (MSE)\n",
        "\n",
        "Answer: We use Mean Squared Error (MSE) to measure the average squared\n",
        "        difference between the actual and predicted values\n",
        "\n",
        "Q5: What does an Adjusted R-squared value of 0.85 indicate\n",
        "\n",
        "Answer: An Adjusted R-squared value of 0.85 means that 85% of the variability  \n",
        "        in the dependent variable is explained by the model, after adjusting for the number of predictors\n",
        "\n",
        "Q6:  How do we check for normality of residuals in linear regression\n",
        "\n",
        "Answer: Histogram: Plot a histogram of the residuals to see if they follow a\n",
        "        bell-shaped curve.\n",
        "\n",
        "        Q-Q Plot: Create a Quantile-Quantile plot to compare the residuals against a normal distribution.\n",
        "\n",
        "        Shapiro-Wilk Test: Perform a statistical test to formally check if residuals are normally distributed\n",
        "\n",
        "Q7: What is multicollinearity, and how does it impact regression\n",
        "\n",
        "Answer: Multicollinearity occurs when two or more independent variables in\n",
        "        a   regression model are highly correlated with each other.\n",
        "\n",
        "Q8: What is Mean Absolute Error (MAE)\n",
        "\n",
        "Answer: Mean Absolute Error (MAE) measures the average of the absolute\n",
        "        differences between actual and predicted values.\n",
        "\n",
        "Q9: What are the benefits of using an ML pipeline\n",
        "\n",
        "Answer: Consistency: Ensures the same steps are followed each time,\n",
        "        reducing    errors.\n",
        "\n",
        "       Efficiency: Speeds up the process by automating repetitive tasks like data preprocessing and model training.\n",
        "\n",
        "       Reproducibility: Makes it easier to reproduce results for future use or testing\n",
        "\n",
        "Q10:  Why is RMSE considered more interpretable than MSE\n",
        "\n",
        "Answer: RMSE (Root Mean Squared Error) is considered more interpretable than  \n",
        "        MSE (Mean Squared Error) because it is in the same units as the original data, making it easier to understand\n",
        "\n",
        "Q11: What is pickling in Python, and how is it useful in ML\n",
        "\n",
        "Answer:  Its useful in ML because it allows you to save trained models, so you\n",
        "         can load and use them later without retraining, saving time and resources.\n",
        "\n",
        "Q12: What does a high R-squared value mean\n",
        "\n",
        "Answer: A high R-squared value means that the model explains a large portion of\n",
        "        the variation in the dependent variable\n",
        "\n",
        "\n",
        "Q13: What happens if linear regression assumptions are violated\n",
        "\n",
        "Answer: Non-linearity: The model wont capture the true relationship.\n",
        "\n",
        "        Non-independence: Errors can become correlated, affecting predictions.\n",
        "\n",
        "        Heteroscedasticity: Uneven spread of errors leads to unreliable estimates.\n",
        "\n",
        "Q14:  How can we address multicollinearity in regression\n",
        "\n",
        "Answer: Remove highly correlated variables: Drop one of the correlated\n",
        "        predictors.\n",
        "\n",
        "        Combine variables: Use techniques like Principal Component Analysis (PCA) to combine correlated variables.\n",
        "\n",
        "        Regularization: Use methods like Ridge or Lasso regression to reduce the impact of multicollinearity.\n",
        "\n",
        "Q15:  How can feature selection improve model performance in regression analysis\n",
        "\n",
        "Answer: Simpler models: Fewer variables make the model easier to understand.\n",
        "\n",
        "        Faster training: Reducing the number of features speeds up the model-building process.\n",
        "\n",
        "Q16:  How is Adjusted R-squared calculated\n",
        "\n",
        "Answer: Adjusted R-squared is calculated using the formula:\n",
        "\n",
        "        Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - p - 1)]\n",
        "\n",
        "Q17: Why is MSE sensitive to outliers\n",
        "\n",
        "Answer: MSE (Mean Squared Error) is sensitive to outliers because it squares  \n",
        "        the differences between actual and predicted values. Larger errors get amplified more due to squaring, meaning outliers (large errors) have a bigger impact on the MSE value.\n",
        "\n",
        "Q18: What is the role of homoscedasticity in linear regression\n",
        "\n",
        "Answer: Homoscedasticity in linear regression means that the variance of the\n",
        "        errors (or residuals) is constant across all levels of the independent variable(s)\n",
        "\n",
        "Q19: What is Root Mean Squared Error (RMSE)\n",
        "\n",
        "Answer:  It calculates the square root of the average squared differences\n",
        "         between predicted and actual values. Lower RMSE means better model performance, as it indicates smaller errors.\n",
        "\n",
        "Q20: Why is pickling considered risky\n",
        "\n",
        "Answer: Pickling is considered risky because it can execute arbitrary code when\n",
        "        loading data, making it vulnerable to malicious attacks.\n",
        "\n",
        "\n",
        "Q21: What alternatives exist to pickling for saving ML models\n",
        "\n",
        "Answer: Joblib: More efficient than pickling, especially for large models.\n",
        "\n",
        "        ONNX: A platform-independent format for saving models, useful for cross-platform deployment.\n",
        "\n",
        "Q22:  What is heteroscedasticity, and why is it a problem\n",
        "\n",
        "Answer: Heteroscedasticity occurs when the variance of errors in a regression\n",
        "        model is not constant across all levels of the independent variable(s). Its a problem because it can lead to inefficient estimates, biased results, and invalid conclusions in hypothesis testing\n",
        "\n",
        "Q23:  How can interaction terms enhance a regression model's predictive power\n",
        "\n",
        "Answer: Interaction terms enhance a regression model by capturing the combined effect of two or more variables on the outcome\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WxJbqHlgv9fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#practical\n",
        "#Q1  Write a Python script to visualize the distribution of errors (residuals)\n",
        "#    for a multiple linear regression modelusing Seaborn's \"diamonds\" dataset\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Q2  Write a Python script to calculate and print Mean Squared Error (MSE),\n",
        " #   Mean Absolute Error (MAE), and RootMean Squared Error (RMSE) for a linear\n",
        " #   regression model.\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "#Q3  Write a Python script to check if the assumptions of linear regression are\n",
        "#    met. Use a scatter plot to checklinearity, residuals plot for\n",
        "#    homoscedasticity, and correlation matrix for multicollinearity\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Load the 'diamonds' dataset from seaborn\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Preprocessing: Use numeric columns for regression (drop categorical 'cut', 'color', 'clarity')\n",
        "diamonds = diamonds.dropna(subset=['price', 'carat', 'depth', 'table', 'x', 'y', 'z'])\n",
        "\n",
        "# Select features and target variable\n",
        "X = diamonds[['carat', 'depth', 'table', 'x', 'y', 'z']]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# 1. Check Linearity: Scatter plots of each feature vs target\n",
        "plt.figure(figsize=(12, 10))\n",
        "for i, feature in enumerate(X.columns):\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    sns.scatterplot(x=X_test[feature], y=y_test)\n",
        "    plt.title(f'Scatter plot: {feature} vs price')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Check Homoscedasticity: Plot residuals vs predicted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.residplot(x=y_pred, y=residuals, lowess=True, line_kws={'color': 'red'})\n",
        "plt.title('Residuals Plot (Homoscedasticity Check)')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()\n",
        "\n",
        "# 3. Check Multicollinearity: Correlation matrix of features\n",
        "plt.figure(figsize=(8, 6))\n",
        "corr_matrix = X.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)\n",
        "plt.title('Correlation Matrix (Multicollinearity Check)')\n",
        "plt.show()\n",
        "\n",
        "#Q4:   Write a Python script that creates a machine learning pipeline with\n",
        "#    feature scaling and evaluates theperformance of different regression models\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the 'diamonds' dataset from seaborn\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Preprocessing: Use numeric columns for regression (drop categorical 'cut', 'color', 'clarity')\n",
        "diamonds = diamonds.dropna(subset=['price', 'carat', 'depth', 'table', 'x', 'y', 'z'])\n",
        "\n",
        "# Select features and target variable\n",
        "X = diamonds[['carat', 'depth', 'table', 'x', 'y', 'z']]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a list of models to evaluate\n",
        "models = [\n",
        "    ('Linear Regression', LinearRegression()),\n",
        "    ('Ridge Regression', Ridge(alpha=1.0)),\n",
        "    ('Lasso Regression', Lasso(alpha=0.1)),\n",
        "    ('Random Forest Regression', RandomForestRegressor(n_estimators=100, random_state=42))\n",
        "]\n",
        "\n",
        "# Create a pipeline with scaling and regression model\n",
        "results = []\n",
        "for model_name, model in models:\n",
        "    # Create a pipeline with scaling and the regression model\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),  # Feature scaling\n",
        "        ('regressor', model)  # Regression model\n",
        "    ])\n",
        "\n",
        "    # Train the model using cross-validation (on training data)\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Store the results\n",
        "    results.append((model_name, mse, r2))\n",
        "\n",
        "# Display results\n",
        "results_df = pd.DataFrame(results, columns=['Model', 'MSE', 'R²'])\n",
        "print(results_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1aOH6t8r1xaz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}